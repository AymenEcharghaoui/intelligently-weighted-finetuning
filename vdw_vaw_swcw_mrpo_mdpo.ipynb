{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------- \n",
    "# Main settings for running experiments\n",
    "# --------------------------------------------------------------------------- \n",
    "# must be one of [\"PKU-SafeRLHF-30K-standard\", \"ultrafeedback_binarized\"]\n",
    "DATASET = \"ultrafeedback_binarized\"\n",
    "# must be one of [0,1,2,3,4]\n",
    "SEED = 2\n",
    "# True for MRPO, False for MDPO\n",
    "USE_MRPO_OVER_MDPO = True\n",
    "# must be one of [\"online_1\", \"offline_1\", \"offline_2\", \"arwc_normalized\"\n",
    "ALPHA_METHOD = \"online_1\"\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- \n",
    "# Directory settings\n",
    "# --------------------------------------------------------------------------- \n",
    "\n",
    "# directoriy where data (preference datasets + precomputed log probs)\n",
    "DATA_DIR = \"/content/drive/MyDrive/mypartcs329h/cleaned/\"\n",
    "# directory to save logs\n",
    "LOGS_DIR = \"/content/drive/MyDrive/mypartcs329h/logs/\"\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- \n",
    "# Model and training hyperparameters\n",
    "# --------------------------------------------------------------------------- \n",
    "# device to use\n",
    "DEVICE = 'cuda:0'\n",
    "# default base model name under HuggingFace\n",
    "BASE_MODEL_PATH = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# whether to use 4-bit quantization\n",
    "use_4bit = True\n",
    "# whether to use LoRA\n",
    "use_lora = True\n",
    "# learning rate, beta (for KL penalty), number of epochs\n",
    "LR, BETA, NUM_EPOCHS = 1e-4, 0.1, 1\n",
    "# set batch size based on dataset\n",
    "BATCH_SIZE = 50 if DATASET == \"PKU-SafeRLHF-30K-standard\" else 25\n",
    "# compute how many batches we will need\n",
    "NUM_BATCHES = (5000 // BATCH_SIZE) if (5000 % BATCH_SIZE) == 0 else (5000 // BATCH_SIZE) + 1\n",
    "# whether to  use micro-batching to fit in GPU memory\n",
    "use_micro_batch = False\n",
    "MICRO_BATCH = 5\n",
    "assert BATCH_SIZE % MICRO_BATCH == 0\n",
    "NUM_MICRO_BATCHES = BATCH_SIZE // MICRO_BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Of2OkJkyMG7s"
   },
   "outputs": [],
   "source": [
    "# This cell installs specific versions of accelerate and bitsandbytes that worked well in our experiments.\n",
    "! pip install accelerate==1.7.0 bitsandbytes==0.46.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYh5-imCBFMf"
   },
   "outputs": [],
   "source": [
    "import sys, os, gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Mount Google Drive to get access to data and save logs/models\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "sys.path.append(\"/content/drive/MyDrive/mypartcs329h\")\n",
    "# Import helper functions we implemented\n",
    "from helpers import CLRL, compute_acc, compute_mrpo_objective, compute_mdpo_objective, compute_dpo_objective_many_refs, expand_vector_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------- \n",
    "# Other settings\n",
    "# --------------------------------------------------------------------------- \n",
    "\n",
    "# precomputed aliases for datasets\n",
    "PRECOMPUTED_ALIASES = {\n",
    "    \"ultrafeedback_binarized\" : \"UltraFeedback\",\n",
    "    \"PKU-SafeRLHF-30K-standard\" : \"SafeRLHF\"\n",
    "}\n",
    "# 7 reference models in MRPO/MDPO\n",
    "REFERENCE_MODELS = [\n",
    "    \"01-ai_Yi-1.5-9B-Chat\",\n",
    "    \"meta-llama_Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"microsoft_Phi-3-medium-128k-instruct\",\n",
    "    \"mistralai_Mistral-7B-Instruct-v0.3\",\n",
    "    \"Qwen_Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen_Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen_Qwen3-4B-Instruct-2507\"\n",
    "]\n",
    "# seeds for experiments - corresponds to different data shuffles\n",
    "SEEDS = [0,1,2,3,4]\n",
    "# offline_1 = VDW\n",
    "# offline_2 = VAW\n",
    "# online_1 = SWCW\n",
    "# arwc_normalized = original\n",
    "ALPHA_METHODS = [\"offline_1\", \"offline_2\", \"online_1\", \"arwc_normalized\"]\n",
    "\n",
    "# assertions to ensure correct settings\n",
    "assert ALPHA_METHOD in ALPHA_METHODS, f\"ALPHA_METHOD {ALPHA_METHOD} not in {ALPHA_METHODS}\"\n",
    "assert DATASET in PRECOMPUTED_ALIASES.keys(), f\"DATASET {DATASET} not in {list(PRECOMPUTED_ALIASES.keys())}\"\n",
    "assert SEED in SEEDS, f\"SEED {SEED} not in {SEEDS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------- \n",
    "# Load preference data and precomputed log probs\n",
    "# --------------------------------------------------------------------------- \n",
    "\n",
    "# preference data (prompt, chosen, rejected)\n",
    "train_df = pd.read_csv(f\"{DATA_DIR}/{DATASET}/seed={SEED}/data/train.csv\")\n",
    "val_df = pd.read_csv(f\"{DATA_DIR}/{DATASET}/seed={SEED}/data/val.csv\")\n",
    "test_df = pd.read_csv(f\"{DATA_DIR}/{DATASET}/seed={SEED}/data/test.csv\")\n",
    "# convert to list of values  \n",
    "val_df_prompts = list(val_df.prompt.values)\n",
    "val_df_chosen = list(val_df.chosen.values)\n",
    "val_df_rejected = list(val_df.rejected.values)\n",
    "test_df_prompts = list(test_df.prompt.values)\n",
    "test_df_chosen = list(test_df.chosen.values)\n",
    "test_df_rejected = list(test_df.rejected.values)\n",
    "# log probs data per split and reference model\n",
    "log_prob_ref_dict = {}\n",
    "for SPLIT in [\"train\", \"val\", \"test\"]:\n",
    "    log_prob_ref_dict[SPLIT] = {}\n",
    "    for REFERENCE_MODEL in REFERENCE_MODELS:\n",
    "        df = pd.read_csv(f\"{DATA_DIR}/{DATASET}/seed={SEED}/precomputed/{REFERENCE_MODEL}_{SPLIT}.csv\")\n",
    "        df.reset_index(inplace=True)\n",
    "        log_prob_ref_dict[SPLIT][REFERENCE_MODEL] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------- \n",
    "# Load in base model + tokenizer\n",
    "# --------------------------------------------------------------------------- \n",
    "\n",
    "# BitsAndBytes + LoRA configurations\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ") if use_4bit else None\n",
    "target_modules = \"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n",
    "lora_cfg = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    target_modules=target_modules.split(\",\"),\n",
    ") if use_lora else None\n",
    "# Load in base model + tokenizer \n",
    "train_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    device_map=DEVICE,\n",
    "    quantization_config=bnb_cfg,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True, use_fast=False)\n",
    "train_model = prepare_model_for_kbit_training(train_model)\n",
    "train_model = get_peft_model(train_model, lora_cfg)\n",
    "train_model.train()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "train_model.config.use_cache = False\n",
    "train_model.gradient_checkpointing_enable()\n",
    "train_model.enable_input_require_grads()\n",
    "train_model.to(DEVICE)\n",
    "# set up optimizer and objective function\n",
    "optimizer = AdamW(train_model.parameters(), lr=LR)\n",
    "compute_objective = compute_mrpo_objective if USE_MRPO_OVER_MDPO else compute_mdpo_objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------- \n",
    "# Compute alphas based on ALPHA_METHOD\n",
    "# --------------------------------------------------------------------------- \n",
    "alphas2use = None\n",
    "if ALPHA_METHOD == 'offline_1':\n",
    "    # dict mapping ref model to alpha value\n",
    "    ref2alpha = {}\n",
    "    for ref_model, log_probs in log_prob_ref_dict['val'].items():\n",
    "        logprob_chosen = log_probs['logprob_chosen'].values\n",
    "        length_chosen = log_probs['L_chosen'].values\n",
    "        logprob_rejected = log_probs['logprob_rejected'].values\n",
    "        length_rejected = log_probs['L_rejected'].values\n",
    "        # compute alpha as sum of absolute differences in length-normalized log probs\n",
    "        alpha = np.sum(np.abs(logprob_chosen/length_chosen - logprob_rejected/length_rejected))\n",
    "        ref2alpha[ref_model] = alpha\n",
    "    total_alpha = sum(ref2alpha.values())\n",
    "    for ref_model in ref2alpha.keys():\n",
    "        # normalize alpha values so they sum to 1\n",
    "        ref2alpha[ref_model] = ref2alpha[ref_model] / total_alpha\n",
    "    # convert to tensor and move to device\n",
    "    alphas_offline_1 = torch.tensor([ref2alpha[x] for x in REFERENCE_MODELS]).to(DEVICE)\n",
    "    alphas2use = alphas_offline_1[None,:]\n",
    "elif ALPHA_METHOD == 'offline_2':\n",
    "    # dict mapping ref model to alpha value\n",
    "    ref2alpha = {}\n",
    "    for ref_model, log_probs in log_prob_ref_dict['val'].items():\n",
    "        logprob_chosen = log_probs['logprob_chosen'].values\n",
    "        length_chosen = log_probs['L_chosen'].values\n",
    "        logprob_rejected = log_probs['logprob_rejected'].values\n",
    "        length_rejected = log_probs['L_rejected'].values\n",
    "        # compute alpha as sum of counts where chosen logprob > rejected logprob\n",
    "        alpha = np.sum((logprob_chosen/length_chosen > logprob_rejected/length_rejected))\n",
    "        ref2alpha[ref_model] = alpha\n",
    "    total_alpha = sum(ref2alpha.values())\n",
    "    for ref_model in ref2alpha.keys():\n",
    "        # normalize alpha values so they sum to 1\n",
    "        ref2alpha[ref_model] = ref2alpha[ref_model] / total_alpha\n",
    "    # convert to tensor and move to device\n",
    "    alphas_offline_2 = torch.tensor([ref2alpha[x] for x in REFERENCE_MODELS]).to(DEVICE)\n",
    "    alphas2use = alphas_offline_2[None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I88f7cHXKFSx"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------- \n",
    "# Training loop \n",
    "# --------------------------------------------------------------------------- \n",
    "\n",
    "# dict to log results\n",
    "logging_results = {'epoch':[], 'batch':[], 'alphas':[], 'batch_acc_pre':[], 'batch_acc_post':[], 'val_acc':[], 'test_acc':[], 'loss': [], 'dpo_by_refs':[]}\n",
    "\n",
    "# initialize alphas2use_from_prev_batch for online_1 as uniform\n",
    "alphas2use_from_prev_batch = torch.ones((1, len(REFERENCE_MODELS)), device=DEVICE) / len(REFERENCE_MODELS)\n",
    "\n",
    "# training loop :\n",
    "for epoch in tqdm(range(NUM_EPOCHS), desc='epoch'):\n",
    "\n",
    "    for batch in tqdm(range(NUM_BATCHES), desc='batch'):\n",
    "\n",
    "        # compute current global step\n",
    "        global_step = epoch * NUM_BATCHES + batch\n",
    "        # log epoch and batch\n",
    "        logging_results['epoch'].append(epoch)\n",
    "        logging_results['batch'].append(batch)\n",
    "\n",
    "        # get current batch data\n",
    "        batch_data = train_df.loc[batch * BATCH_SIZE : ((batch+1) * BATCH_SIZE) - 1]\n",
    "        # extract prompts, chosen, rejected\n",
    "        batch_prompt, batch_chosen, batch_rejected = batch_data[\"prompt\"].tolist(), batch_data[\"chosen\"].tolist(), batch_data[\"rejected\"].tolist()\n",
    "\n",
    "        # stack reference log probs for this batch\n",
    "        def stack_column(col_name):\n",
    "            arrs = [\n",
    "                torch.tensor(\n",
    "                    data_model.loc[batch_data.index][col_name].values,\n",
    "                    dtype=torch.float16     \n",
    "                )\n",
    "                for _, data_model in log_prob_ref_dict[\"train\"].items()\n",
    "            ]\n",
    "            return torch.stack(arrs, dim=0).T.to(DEVICE)\n",
    "        # preferred and non-preferred log probs and lengths\n",
    "        preferred_ref_log_probs = stack_column(\"logprob_chosen\")\n",
    "        nonpreferred_ref_log_probs = stack_column(\"logprob_rejected\")\n",
    "        L_chosen_refs           = stack_column(\"L_chosen\")\n",
    "        L_rejected_refs         = stack_column(\"L_rejected\")\n",
    "\n",
    "        # compute accuracy on this batch BEFORE we do the gradient update!\n",
    "        with torch.no_grad():\n",
    "            batch_acc_pre = compute_acc(model=train_model, tok=tok, prompt=batch_prompt, chosen=batch_chosen, rejected=batch_rejected, MAX_BATCH=200, device=DEVICE)\n",
    "            logging_results['batch_acc_pre'].append(batch_acc_pre.detach().cpu().numpy())\n",
    "\n",
    "        # compute alphas to use for this batch unless offline\n",
    "        # original \n",
    "        if ALPHA_METHOD == 'arwc_normalized':\n",
    "            # compute alphas based on normalized absolute differences \n",
    "            alphas = torch.abs(preferred_ref_log_probs/L_chosen_refs-nonpreferred_ref_log_probs/L_rejected_refs)\n",
    "            alphas2use = alphas / torch.sum(alphas, dim=1, keepdim=True) # B x K\n",
    "        # online 1\n",
    "        elif ALPHA_METHOD == 'online_1':\n",
    "            # computed on previous batch\n",
    "            alphas2use = alphas2use_from_prev_batch # B x K\n",
    "        # log alphas used this batch\n",
    "        logging_results['alphas'].append(alphas2use.mean(0).detach().cpu().numpy())\n",
    "\n",
    "        # training step depending on micro-batching\n",
    "        if not use_micro_batch :\n",
    "\n",
    "            # compute the current log-probs on the training batch's preferred.\n",
    "            preferred_train_log_probs, L_chosen = CLRL(\n",
    "                model=train_model, tok=tok, prompts=batch_prompt, replies=batch_chosen, device=DEVICE)\n",
    "\n",
    "            # compute the current log-probs on the training batch's rejected.\n",
    "            nonpreferred_train_log_probs, L_rejected = CLRL(\n",
    "                model=train_model, tok=tok, prompts=batch_prompt, replies=batch_rejected, device=DEVICE)\n",
    "\n",
    "            # B tensors\n",
    "            preferred_train_log_probs = torch.stack(preferred_train_log_probs)\n",
    "            nonpreferred_train_log_probs = torch.stack(nonpreferred_train_log_probs)\n",
    "\n",
    "            # use length normalization\n",
    "            L_chosen = torch.from_numpy(np.asarray(L_chosen)).to(DEVICE)\n",
    "            L_rejected = torch.from_numpy(np.asarray(L_rejected)).to(DEVICE)\n",
    "            # length-normalized log probs both train and ref\n",
    "            preferred_train_log_probs_2use = preferred_train_log_probs / L_chosen\n",
    "            nonpreferred_train_log_probs_2use = nonpreferred_train_log_probs / L_rejected\n",
    "            preferred_ref_log_probs_2use = preferred_ref_log_probs / L_chosen_refs\n",
    "            nonpreferred_ref_log_probs_2use = nonpreferred_ref_log_probs / L_rejected_refs\n",
    "\n",
    "            # forward-pass\n",
    "            loss = compute_objective(\n",
    "                preferred_train_log_probs_2use,\n",
    "                nonpreferred_train_log_probs_2use,\n",
    "                preferred_ref_log_probs_2use,\n",
    "                nonpreferred_ref_log_probs_2use,\n",
    "                beta=BETA, alphas=alphas2use)\n",
    "\n",
    "            # backward-pass\n",
    "            loss.backward()\n",
    "            # log loss\n",
    "            logging_results['loss'].append(loss.item())\n",
    "            # print loss\n",
    "            print(f\"Epoch {epoch}, Batch {batch}, Loss: {loss.item()}\")\n",
    "\n",
    "            # update our parameters + zero our gradient\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # compute accuracy on this batch after training\n",
    "                batch_acc_post = compute_acc(model=train_model, tok=tok, prompt=batch_prompt, chosen=batch_chosen, rejected=batch_rejected, MAX_BATCH=200, device=DEVICE)\n",
    "                logging_results['batch_acc_post'].append(batch_acc_post.detach().cpu().numpy())\n",
    "\n",
    "                # compute mdpo by ref logs\n",
    "                mdpo_losses = compute_dpo_objective_many_refs(\n",
    "                    preferred_train_log_probs_2use,\n",
    "                    nonpreferred_train_log_probs_2use,\n",
    "                    preferred_ref_log_probs_2use,\n",
    "                    nonpreferred_ref_log_probs_2use,\n",
    "                    beta=BETA) # K\n",
    "                logging_results['dpo_by_refs'].append(mdpo_losses.detach().cpu().numpy())\n",
    "\n",
    "        # micro-batching\n",
    "        else : \n",
    "            micro_losses = []\n",
    "            dpo_by_refs_accum = []\n",
    "            # ensure alphas2use is B x K\n",
    "            if alphas2use.shape[0] == 1:\n",
    "                alphas2use = alphas2use.repeat(BATCH_SIZE, 1)\n",
    "            # iterate over micro-batches\n",
    "            for micro_batch in tqdm(range(NUM_MICRO_BATCHES), desc='micro_batch'):\n",
    "                # get micro-batch data : prompt, chosen, rejected, alphas, ref log probs, lengths\n",
    "                micro_batch_prompt = batch_prompt[micro_batch*MICRO_BATCH:(micro_batch+1)*MICRO_BATCH]\n",
    "                micro_batch_chosen = batch_chosen[micro_batch*MICRO_BATCH:(micro_batch+1)*MICRO_BATCH]\n",
    "                micro_batch_rejected = batch_rejected[micro_batch*MICRO_BATCH:(micro_batch+1)*MICRO_BATCH]\n",
    "                micro_alphas2use = alphas2use[micro_batch*MICRO_BATCH:(micro_batch+1)*MICRO_BATCH]\n",
    "                micro_preferred_ref_log_probs = preferred_ref_log_probs[micro_batch*MICRO_BATCH:(micro_batch+1)*MICRO_BATCH]\n",
    "                micro_nonpreferred_ref_log_probs = nonpreferred_ref_log_probs[micro_batch*MICRO_BATCH:(micro_batch+1)*MICRO_BATCH]\n",
    "                micro_L_chosen_refs = L_chosen_refs[micro_batch*MICRO_BATCH:(micro_batch+1)*MICRO_BATCH]\n",
    "                micro_L_rejected_refs = L_rejected_refs[micro_batch*MICRO_BATCH:(micro_batch+1)*MICRO_BATCH]\n",
    "\n",
    "            # compute the current log-probs on the training batch's preferred.\n",
    "            micro_preferred_train_log_probs, micro_L_chosen = CLRL(\n",
    "                model=train_model, tok=tok, prompts=micro_batch_prompt, replies=micro_batch_chosen, device=DEVICE)\n",
    "\n",
    "            # compute the current log-probs on the training batch's rejected.\n",
    "            micro_nonpreferred_train_log_probs, micro_L_rejected = CLRL(\n",
    "                model=train_model, tok=tok, prompts=micro_batch_prompt, replies=micro_batch_rejected, device=DEVICE)\n",
    "\n",
    "            # B tensors\n",
    "            micro_preferred_train_log_probs = torch.stack(micro_preferred_train_log_probs)\n",
    "            micro_nonpreferred_train_log_probs = torch.stack(micro_nonpreferred_train_log_probs)\n",
    "\n",
    "            # use length normalization\n",
    "            micro_L_chosen = torch.from_numpy(np.asarray(micro_L_chosen)).to(DEVICE)\n",
    "            micro_L_rejected = torch.from_numpy(np.asarray(micro_L_rejected)).to(DEVICE)\n",
    "            micro_preferred_train_log_probs_2use = micro_preferred_train_log_probs / micro_L_chosen\n",
    "            micro_nonpreferred_train_log_probs_2use = micro_nonpreferred_train_log_probs / micro_L_rejected\n",
    "            micro_preferred_ref_log_probs_2use = micro_preferred_ref_log_probs / micro_L_chosen_refs\n",
    "            micro_nonpreferred_ref_log_probs_2use = micro_nonpreferred_ref_log_probs / micro_L_rejected_refs\n",
    "\n",
    "            # forward-pass\n",
    "            loss = compute_objective(\n",
    "                micro_preferred_train_log_probs_2use,\n",
    "                micro_nonpreferred_train_log_probs_2use,\n",
    "                micro_preferred_ref_log_probs_2use,\n",
    "                micro_nonpreferred_ref_log_probs_2use,\n",
    "                beta=BETA, alphas=micro_alphas2use) / NUM_MICRO_BATCHES\n",
    "\n",
    "            # backward-pass\n",
    "            loss.backward()\n",
    "            micro_losses.append(loss.item())\n",
    "            # print loss\n",
    "            print(f\"Epoch {epoch}, Batch {batch}, micro_batch {micro_batch}, Loss: {loss.item()}\")\n",
    "\n",
    "            # compute dpo by ref logs for this micro-batch\n",
    "            with torch.no_grad():\n",
    "                mb_dpo_losses = compute_dpo_objective_many_refs(\n",
    "                    micro_preferred_train_log_probs_2use,   # (m)\n",
    "                    micro_nonpreferred_train_log_probs_2use,\n",
    "                    micro_preferred_ref_log_probs_2use,\n",
    "                    micro_nonpreferred_ref_log_probs_2use,\n",
    "                    beta=BETA\n",
    "                )\n",
    "                dpo_by_refs_accum.append(mb_dpo_losses)\n",
    "\n",
    "            # update our parameters + zero our gradient\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            logging_results['loss'].append(np.sum(micro_losses))\n",
    "\n",
    "            # aggregate dpo by refs for this batch\n",
    "            dpo_by_refs_full = torch.vstack(dpo_by_refs_accum)   # (B, K)\n",
    "            logging_results[\"dpo_by_refs\"].append(\n",
    "                dpo_by_refs_full.mean(0).detach().cpu().numpy()\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # compute accuracy on this batch after training\n",
    "                batch_acc_post = compute_acc(model=train_model, tok=tok, prompt=batch_prompt, chosen=batch_chosen, rejected=batch_rejected, MAX_BATCH=200, device=DEVICE)\n",
    "                logging_results['batch_acc_post'].append(batch_acc_post.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "        # compute alphas for online 1 this batch will be used next batch\n",
    "        if ALPHA_METHOD == 'online_1': # computed on previous batch\n",
    "            alphas = torch.abs(preferred_ref_log_probs/L_chosen_refs-nonpreferred_ref_log_probs/L_rejected_refs)\n",
    "            alphas2use_from_prev_batch = alphas / torch.sum(alphas, dim=1, keepdim=True) # B x K\n",
    "\n",
    "        # compute val and test acc every 10% of training\n",
    "        if (((batch+1)% (NUM_BATCHES//10)==0) or (batch == (NUM_BATCHES-1))):\n",
    "            with torch.no_grad():\n",
    "                # clear cache before computing val and test accuracy\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.ipc_collect()\n",
    "                gc.collect()\n",
    "                # val acc\n",
    "                val_acc = compute_acc(model=train_model, tok=tok, prompt=val_df_prompts, chosen=val_df_chosen, rejected=val_df_rejected, MAX_BATCH=100, device=DEVICE)\n",
    "                logging_results['val_acc'].append(val_acc)\n",
    "                # test acc\n",
    "                test_acc = compute_acc(model=train_model, tok=tok, prompt=test_df_prompts, chosen=test_df_chosen, rejected=test_df_rejected, MAX_BATCH=100, device=DEVICE)\n",
    "                logging_results['test_acc'].append(test_acc)\n",
    "        else:\n",
    "            val_acc, test_acc = None, None\n",
    "            logging_results['val_acc'].append(None)\n",
    "            logging_results['test_acc'].append(None)\n",
    "\n",
    "        # save intermediate logs every 10% of training\n",
    "        if (((batch+1)% (NUM_BATCHES//10)==0) or (batch == (NUM_BATCHES-1))):\n",
    "            df = expand_vector_columns(logging_results, REFERENCE_MODELS)\n",
    "            fname = LOGS_DIR+f\"training_logs_{PRECOMPUTED_ALIASES[DATASET]}_{ALPHA_METHOD}_{SEED}_MRPO.csv\" if USE_MRPO_OVER_MDPO else LOGS_DIR+f\"training_logs_{PRECOMPUTED_ALIASES[DATASET]}_{ALPHA_METHOD}_{SEED}_MDPO.csv\"\n",
    "            df.to_csv(fname, index=False)\n",
    "\n",
    "# save final logs after training\n",
    "df = expand_vector_columns(logging_results, REFERENCE_MODELS)\n",
    "fname = LOGS_DIR+f\"training_logs_{PRECOMPUTED_ALIASES[DATASET]}_{ALPHA_METHOD}_{SEED}_MRPO.csv\" if USE_MRPO_OVER_MDPO else LOGS_DIR+f\"training_logs_{PRECOMPUTED_ALIASES[DATASET]}_{ALPHA_METHOD}_{SEED}_MDPO.csv\"\n",
    "df.to_csv(fname, index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
