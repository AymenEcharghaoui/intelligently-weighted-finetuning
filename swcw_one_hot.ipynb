{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDMs0k7YLInu",
        "outputId": "fea837c0-0abc-48d6-b957-7dbe2b0b83ba"
      },
      "outputs": [],
      "source": [
        "! pip install bitsandbytes==0.46.0 accelerate==1.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uYvE9T6Eall",
        "outputId": "36e91e60-4dba-4fe1-9219-3386e8a67062"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from transformers import get_scheduler\n",
        "from tqdm.notebook import tqdm\n",
        "import sys, os, gc\n",
        "from IPython.display import clear_output\n",
        "from torch.optim import AdamW\n",
        "from peft import PeftConfig, PeftModel, LoraConfig, get_peft_model\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "# mount google drive + link to the correct folder\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "sys.path.append(\"/content/drive/MyDrive/cs329h_project\")\n",
        "\n",
        "# helper functions\n",
        "from helpers import compute_logprob_and_reply_length_batched as CLRL\n",
        "from helpers import compute_acc\n",
        "from helpers import compute_mrpo_objective\n",
        "from helpers import compute_dpo_objective_many_refs\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "# we will try 2x datasets, 3x seeds, with 7x potential reference models\n",
        "DATASETS = [\n",
        "    \"PKU-SafeRLHF-30K-standard\",\n",
        "    \"ultrafeedback_binarized\"]\n",
        "REF_MODELS = [\n",
        "    \"01-ai_Yi-1.5-9B-Chat\",\n",
        "    \"meta-llama_Meta-Llama-3.1-8B-Instruct\",\n",
        "    \"microsoft_Phi-3-medium-128k-instruct\",\n",
        "    \"mistralai_Mistral-7B-Instruct-v0.3\",\n",
        "    \"Qwen_Qwen2.5-0.5B-Instruct\",\n",
        "    \"Qwen_Qwen2.5-1.5B-Instruct\",\n",
        "    \"Qwen_Qwen3-4B-Instruct-2507\"\n",
        "    ]\n",
        "N_REF_MODELS = len(REF_MODELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 691,
          "referenced_widgets": [
            "77e4868c4c6141828c88d8a873b39a0e"
          ]
        },
        "id": "RVbHvGrC4Xl2",
        "outputId": "0f72876a-f21d-4710-e553-ee52de353b02"
      },
      "outputs": [],
      "source": [
        "# keep a counter for checkpointing\n",
        "counter = 0\n",
        "\n",
        "# loop through prior_init_vals, datasets, and seeds\n",
        "for dataset in DATASETS:\n",
        "\n",
        "  # different numbers of seeds for each dataset\n",
        "  SEEDS = range(5) if dataset == \"PKU-SafeRLHF-30K-standard\" else range(3)\n",
        "\n",
        "  # go thru our seeds\n",
        "  for seed in SEEDS:\n",
        "\n",
        "    #### LOADING IN THE TRAINING MODEL\n",
        "\n",
        "    ## configuring BitsandBytes (4bit) + LORA (default) settings.\n",
        "    bnb_cfg = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=False, bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "    target_modules = \"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=32, lora_alpha=16, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
        "        init_lora_weights=\"gaussian\", target_modules=target_modules.split(\",\"))\n",
        "\n",
        "    ## getting our base model Qwen2.5-0.5B-Instruct\n",
        "    POLICY_MODEL = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "    train_model = AutoModelForCausalLM.from_pretrained(\n",
        "        POLICY_MODEL, device_map=\"auto\", quantization_config=bnb_cfg, trust_remote_code=True)\n",
        "    train_model.config.use_cache = False\n",
        "    train_model = prepare_model_for_kbit_training(train_model)\n",
        "    train_model = get_peft_model(train_model, lora_cfg)\n",
        "    train_model.train()\n",
        "    tok = AutoTokenizer.from_pretrained(\n",
        "        POLICY_MODEL, trust_remote_code=True, use_fast=True)\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # some other settings to reduce memory\n",
        "    train_model.gradient_checkpointing_enable()\n",
        "    train_model.enable_input_require_grads()\n",
        "\n",
        "    #### HYPERPARAMETER SETTINGS\n",
        "\n",
        "    # fixed hyperparameter settings\n",
        "    BETA, N_EPOCHS, LR = 0.1, 1, 1e-4\n",
        "    BATCH_SIZE = 50 if dataset == \"PKU-SafeRLHF-30K-standard\" else 25\n",
        "\n",
        "    # compute how many batches we will need\n",
        "    NUM_BATCHES = (5000 // BATCH_SIZE) if (5000 % BATCH_SIZE) == 0 else (5000 // BATCH_SIZE) + 1\n",
        "\n",
        "    # what logs are we trying to record? NO TRAIN_ACC BECAUSE TOO EXPENSIVE\n",
        "    logs = pd.DataFrame(\n",
        "        data=None, columns=[\n",
        "            \"epoch\", \"batch\",\n",
        "            \"batch_acc_pre\", \"batch_acc_post\",\n",
        "            \"chosen_dpo_loss\", \"chosen_reference\",\n",
        "            \"dpo0\", \"dpo1\", \"dpo2\", \"dpo3\", \"dpo4\", \"dpo5\", \"dpo6\",\n",
        "            \"val_acc\", \"test_acc\"])\n",
        "\n",
        "    # get our optimizer\n",
        "    optimizer = AdamW(train_model.parameters(), lr=LR)\n",
        "\n",
        "    #### DATA LOADING\n",
        "\n",
        "    # load in the train/val/test splits for this seed + the pre-computed logits\n",
        "    CLEANED_DIR = f\"/content/drive/MyDrive/cs329h_project/cleaned/{dataset}/seed={seed}\"\n",
        "\n",
        "    # the train/val/test data for this split\n",
        "    train_df = pd.read_csv(f\"{CLEANED_DIR}/data/train.csv\")[\n",
        "        [\"prompt\", \"chosen\", \"rejected\"]]\n",
        "    val_df = pd.read_csv(f\"{CLEANED_DIR}/data/val.csv\")[\n",
        "        [\"prompt\", \"chosen\", \"rejected\"]]\n",
        "    test_df = pd.read_csv(f\"{CLEANED_DIR}/data/test.csv\")[\n",
        "        [\"prompt\", \"chosen\", \"rejected\"]]\n",
        "\n",
        "    # load in all seven models' reference log-probs for this train split\n",
        "    all_ref_train = {\n",
        "        i : pd.read_csv(f\"{CLEANED_DIR}/precomputed/{ref_model}_train.csv\")\n",
        "        for (i, ref_model) in enumerate(REF_MODELS)}\n",
        "\n",
        "    #### MODEL TRAINING\n",
        "\n",
        "    # set a seed for reproducibility on the Thompson Sampling\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # initialize the prev_sliding window weights for Offline 1\n",
        "    prev_sliding_window_weights = None\n",
        "\n",
        "    # training loop over epochs and batches\n",
        "    for epoch in range(N_EPOCHS):\n",
        "      for batch in tqdm(range((NUM_BATCHES))):\n",
        "\n",
        "        # get this mini-batch worth of data\n",
        "        batch_data = train_df.loc[batch * BATCH_SIZE : ((batch+1) * BATCH_SIZE) - 1]\n",
        "        batch_prompt, batch_chosen, batch_rejected = batch_data[\"prompt\"].tolist(), batch_data[\"chosen\"].tolist(), batch_data[\"rejected\"].tolist()\n",
        "\n",
        "        # compute accuracy on this batch before we do the gradient update\n",
        "        with torch.no_grad():\n",
        "          batch_acc_pre = compute_acc(\n",
        "              model=train_model, tok=tok, prompt=batch_prompt, chosen=batch_chosen, rejected=batch_rejected, MAX_BATCH=50).cpu().item()\n",
        "\n",
        "        # which reference model are we going to be aligning to?\n",
        "        if prev_sliding_window_weights is not None:\n",
        "          chosen_reference = np.argmax(prev_sliding_window_weights)\n",
        "        else:\n",
        "          chosen_reference = 4 # Qwen-2.5-0.5B-Instruct\n",
        "\n",
        "        # lists to store our snippets of policy_lpm_chosen and rejected, also our loss\n",
        "        policy_lpm_chosen, policy_lpm_rejected = [], []\n",
        "        loss = 0.0\n",
        "\n",
        "        # micro-batch in sizes of 13\n",
        "        MICRO_BATCH_SIZE = 16\n",
        "        N_MICRO_BATCHES = (BATCH_SIZE // MICRO_BATCH_SIZE) if BATCH_SIZE % MICRO_BATCH_SIZE == 0 else (BATCH_SIZE // MICRO_BATCH_SIZE) + 1\n",
        "        for m_batch in range(N_MICRO_BATCHES):\n",
        "\n",
        "          # what are the indices for this mini-batch?\n",
        "          m_batch_start, m_batch_end = m_batch * MICRO_BATCH_SIZE, np.minimum(BATCH_SIZE, (m_batch + 1) * MICRO_BATCH_SIZE)\n",
        "\n",
        "          # compute the current log-probs on the training batch's preferred.\n",
        "          preferred_train_log_probs_m, L_chosen_m = CLRL(\n",
        "              model=train_model, tok=tok,\n",
        "              prompts=batch_prompt[m_batch_start : m_batch_end],\n",
        "              replies=batch_chosen[m_batch_start : m_batch_end], device=device)\n",
        "          policy_lpm_chosen_m = torch.stack(preferred_train_log_probs_m) / torch.tensor(L_chosen_m, device=device, dtype=torch.float16)\n",
        "\n",
        "          # compute the current log-probs on the training batch's rejected.\n",
        "          nonpreferred_train_log_probs_m, L_rejected_m = CLRL(\n",
        "              model=train_model, tok=tok,\n",
        "              prompts=batch_prompt[m_batch_start : m_batch_end],\n",
        "              replies=batch_rejected[m_batch_start : m_batch_end], device=device)\n",
        "          policy_lpm_rejected_m = torch.stack(nonpreferred_train_log_probs_m) / torch.tensor(L_rejected_m, device=device, dtype=torch.float16)\n",
        "\n",
        "          # get the logprobs and L of the CHOSEN precomputed reference model\n",
        "          ref_lp_chosen_m = torch.tensor(\n",
        "              all_ref_train[chosen_reference].loc[batch_data.index][\"logprob_chosen\"].values[m_batch_start : m_batch_end], dtype=torch.float16).to(device)\n",
        "          ref_lp_rejected_m = torch.tensor(\n",
        "              all_ref_train[chosen_reference].loc[batch_data.index][\"logprob_rejected\"].values[m_batch_start : m_batch_end], dtype=torch.float16).to(device)\n",
        "          ref_L_chosen_m = torch.tensor(\n",
        "              all_ref_train[chosen_reference].loc[batch_data.index][\"L_chosen\"].values[m_batch_start : m_batch_end], dtype=torch.float16).to(device)\n",
        "          ref_L_rejected_m = torch.tensor(\n",
        "              all_ref_train[chosen_reference].loc[batch_data.index][\"L_rejected\"].values[m_batch_start : m_batch_end], dtype=torch.float16).to(device)\n",
        "          ref_lpm_chosen_m = ref_lp_chosen_m / ref_L_chosen_m\n",
        "          ref_lpm_rejected_m = ref_lp_rejected_m / ref_L_rejected_m\n",
        "\n",
        "          # compute the DPO objective\n",
        "          loss_m = torch.mean(\n",
        "              -torch.nn.functional.logsigmoid(\n",
        "                  BETA * (\n",
        "                      (policy_lpm_chosen_m - ref_lpm_chosen_m) - \\\n",
        "                      (policy_lpm_rejected_m - ref_lpm_rejected_m))\n",
        "                  )\n",
        "              ) * (m_batch_end - m_batch_start) / BATCH_SIZE\n",
        "\n",
        "          # backward-pass\n",
        "          loss_m.backward()\n",
        "\n",
        "          # add detached versions of policy_lpm_chosen_m, policy_lpm_rejected_m\n",
        "          policy_lpm_chosen.append(policy_lpm_chosen_m.detach())\n",
        "          policy_lpm_rejected.append(policy_lpm_rejected_m.detach())\n",
        "          loss += loss_m.detach()\n",
        "\n",
        "        # update our parameters + zero our gradient for the full batch!\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # get the full tensors\n",
        "        policy_lpm_chosen = torch.cat(policy_lpm_chosen, dim=0)\n",
        "        policy_lpm_rejected = torch.cat(policy_lpm_rejected, dim=0)\n",
        "\n",
        "        # compute accuracy on this batch AFTER we did the gradient update\n",
        "        with torch.no_grad():\n",
        "          batch_acc_post = compute_acc(\n",
        "              model=train_model, tok=tok, prompt=batch_prompt, chosen=batch_chosen, rejected=batch_rejected, MAX_BATCH=50).cpu().item()\n",
        "\n",
        "        # record 10 batches and guarantee the ending.\n",
        "        if ((batch + 1) % (NUM_BATCHES // 10) == 0) or (batch == (NUM_BATCHES - 1)):\n",
        "\n",
        "          # compute val and test accuracy too.\n",
        "          with torch.no_grad():\n",
        "\n",
        "            # validation set\n",
        "            val_acc = compute_acc(\n",
        "                train_model, tok, val_df.prompt.values, val_df.chosen.values,\n",
        "                val_df.rejected.values, MAX_BATCH=50).cpu().item()\n",
        "\n",
        "            # test set acc\n",
        "            test_acc = compute_acc(\n",
        "                train_model, tok, test_df.prompt.values, test_df.chosen.values,\n",
        "                test_df.rejected.values, MAX_BATCH=50).cpu().item()\n",
        "            print(test_acc)\n",
        "\n",
        "        else:\n",
        "\n",
        "          # just put nan's.\n",
        "          val_acc, test_acc = np.nan, np.nan\n",
        "\n",
        "        # also record per-reference model DPO losses\n",
        "        all_ref_lp_chosen = torch.tensor(\n",
        "            np.array([all_ref_train[i].loc[batch_data.index][\"logprob_chosen\"].values\n",
        "                      for i in range(N_REF_MODELS)]).T, dtype=torch.float16).to(device)\n",
        "        all_ref_lp_rejected = torch.tensor(\n",
        "            np.array([all_ref_train[i].loc[batch_data.index][\"logprob_rejected\"].values\n",
        "                      for i in range(N_REF_MODELS)]).T, dtype=torch.float16).to(device)\n",
        "        all_ref_L_chosen = torch.tensor(\n",
        "            np.array([all_ref_train[i].loc[batch_data.index][\"L_chosen\"].values\n",
        "                      for i in range(N_REF_MODELS)]).T, dtype=torch.float16).to(device)\n",
        "        all_ref_L_rejected = torch.tensor(\n",
        "            np.array([all_ref_train[i].loc[batch_data.index][\"L_rejected\"].values\n",
        "                      for i in range(N_REF_MODELS)]).T, dtype=torch.float16).to(device)\n",
        "        all_ref_lpm_chosen = all_ref_lp_chosen / all_ref_L_chosen\n",
        "        all_ref_lpm_rejected = all_ref_lp_rejected / all_ref_L_rejected\n",
        "        with torch.no_grad():\n",
        "          per_reference_dpo_loss = compute_dpo_objective_many_refs(\n",
        "              policy_lpm_chosen, policy_lpm_rejected,\n",
        "              all_ref_lpm_chosen, all_ref_lpm_rejected, BETA)\n",
        "\n",
        "        # record our results\n",
        "        logs.loc[len(logs.index)] = [\n",
        "            epoch, batch,\n",
        "            batch_acc_pre, batch_acc_post,\n",
        "            loss.cpu().item(), chosen_reference]\\\n",
        "              + list(per_reference_dpo_loss.cpu().numpy())\\\n",
        "              + [val_acc, test_acc]\n",
        "\n",
        "        # compute our previous sliding window weights for Offline 1.\n",
        "        prev_sliding_window_weights = torch.abs(all_ref_lpm_chosen - all_ref_lpm_rejected).mean(dim=0).cpu().numpy()\n",
        "\n",
        "        # clean house\n",
        "        del batch_data, batch_prompt, batch_chosen, batch_rejected, batch_acc_pre\n",
        "        del preferred_train_log_probs_m, L_chosen_m, policy_lpm_chosen_m, policy_lpm_chosen\n",
        "        del nonpreferred_train_log_probs_m, L_rejected_m, policy_lpm_rejected_m, policy_lpm_rejected\n",
        "        del chosen_reference, ref_lp_chosen_m, ref_lp_rejected_m, ref_L_chosen_m, ref_L_rejected_m, ref_lpm_chosen_m, ref_lpm_rejected_m\n",
        "        del loss_m, loss, batch_acc_post, val_acc, test_acc\n",
        "        del all_ref_lp_chosen, all_ref_lp_rejected, all_ref_L_chosen, all_ref_L_rejected, all_ref_lpm_chosen, all_ref_lpm_rejected\n",
        "        del per_reference_dpo_loss\n",
        "        gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    # save our logs at the end\n",
        "    logs.to_csv(\n",
        "      f\"/content/drive/MyDrive/cs329h_project/results/Offline-1-One-Hot_dataset={dataset}_seed={seed}.csv\", index=False)\n",
        "\n",
        "    #### CLEAN HOUSE + STATUS UPDATE\n",
        "\n",
        "    # update our counter\n",
        "    counter += 1\n",
        "\n",
        "    # clear memory\n",
        "    del bnb_cfg, target_modules, lora_cfg, train_model, tok, logs, optimizer\n",
        "    del train_df, val_df, test_df, all_ref_train\n",
        "    gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "    # clear output too\n",
        "    clear_output(wait=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
