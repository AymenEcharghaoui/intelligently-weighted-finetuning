{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDMs0k7YLInu",
        "outputId": "681d0e0a-2828-4304-b467-893ecd542729"
      },
      "outputs": [],
      "source": [
        "! pip install bitsandbytes==0.46.0 accelerate==1.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uYvE9T6Eall",
        "outputId": "ae78c8eb-84cf-4e34-d20b-e1edc1ed81c7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from transformers import get_scheduler\n",
        "from tqdm.notebook import tqdm\n",
        "import sys, os, gc\n",
        "from IPython.display import clear_output\n",
        "from torch.optim import AdamW\n",
        "from peft import PeftConfig, PeftModel, LoraConfig, get_peft_model\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "# mount google drive + link to the correct folder\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "sys.path.append(\"/content/drive/MyDrive/cs329h_project\")\n",
        "\n",
        "# helper functions\n",
        "from helpers import compute_logprob_and_reply_length_batched as CLRL\n",
        "from helpers import compute_acc\n",
        "from helpers import compute_mrpo_objective\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# settings to loop over\n",
        "DATASETS = [\n",
        "    # \"helpsteer2-preference-v2\",\n",
        "    \"PKU-SafeRLHF-30K-standard\",\n",
        "    \"ultrafeedback_binarized\"]\n",
        "REF_MODELS = [\n",
        "    \"01-ai_Yi-1.5-9B-Chat\",\n",
        "    \"meta-llama_Meta-Llama-3.1-8B-Instruct\",\n",
        "    \"microsoft_Phi-3-medium-128k-instruct\",\n",
        "    \"mistralai_Mistral-7B-Instruct-v0.3\",\n",
        "    \"Qwen_Qwen2.5-0.5B-Instruct\",\n",
        "    \"Qwen_Qwen2.5-1.5B-Instruct\",\n",
        "    \"Qwen_Qwen3-4B-Instruct-2507\"\n",
        "    ]\n",
        "SEEDS = list(range(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 638,
          "referenced_widgets": [
            "9f7f3ce14e2e491fad6f0b84a8583ee1"
          ]
        },
        "id": "1dHZhHmkpa_7",
        "outputId": "e65c066e-73eb-45c7-c45a-b4fe810e3137"
      },
      "outputs": [],
      "source": [
        "# keep a counter for checkpointing\n",
        "counter = 0\n",
        "\n",
        "# loop through our REF_MODELS, DATASETS, SEEDS\n",
        "for dataset in DATASETS:\n",
        "  for ref_model in REF_MODELS:\n",
        "    for seed in SEEDS: \n",
        "      # LOADING IN THE TRAINING MODEL\n",
        "\n",
        "      ## configuring BitsandBytes (4bit) + LORA (default) settings.\n",
        "      bnb_cfg = BitsAndBytesConfig(\n",
        "          load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
        "          bnb_4bit_use_double_quant=False, bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "      target_modules = \"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n",
        "      lora_cfg = LoraConfig(\n",
        "          r=32, lora_alpha=16, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
        "          init_lora_weights=\"gaussian\", target_modules=target_modules.split(\",\"))\n",
        "\n",
        "      ## getting our base model Qwen2.5-0.5B-Instruct\n",
        "      POLICY_MODEL = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "      train_model = AutoModelForCausalLM.from_pretrained(\n",
        "          POLICY_MODEL, device_map=\"auto\", quantization_config=bnb_cfg, trust_remote_code=True)\n",
        "      train_model.config.use_cache = False\n",
        "      train_model = prepare_model_for_kbit_training(train_model)\n",
        "      train_model = get_peft_model(train_model, lora_cfg)\n",
        "      train_model.train()\n",
        "      tok = AutoTokenizer.from_pretrained(\n",
        "          POLICY_MODEL, trust_remote_code=True, use_fast=True)\n",
        "      torch.cuda.empty_cache()\n",
        "      gc.collect()\n",
        "\n",
        "      # some other settings to reduce memory\n",
        "      train_model.gradient_checkpointing_enable()\n",
        "      train_model.enable_input_require_grads()\n",
        "\n",
        "      #### HYPERPARAMETER SETTINGS\n",
        "\n",
        "      # fixed hyperparameter settings\n",
        "      BETA, N_EPOCHS, LR = 0.1, 1, 1e-4\n",
        "      BATCH_SIZE = 50 if dataset == \"PKU-SafeRLHF-30K-standard\" else 25 # let's use batchsize 25 for ultra-feedback\n",
        "\n",
        "      # compute how many batches we will need\n",
        "      NUM_BATCHES = (5000 // BATCH_SIZE) if (5000 % BATCH_SIZE) == 0 else (5000 // BATCH_SIZE) + 1\n",
        "\n",
        "      # what logs are we trying to record? NO TRAIN_ACC BECAUSE TOO EXPENSIVE\n",
        "      logs = pd.DataFrame(\n",
        "          data=None, columns=[\n",
        "              \"epoch\", \"batch\", \"batch_acc_pre\", \"batch_acc_post\",\n",
        "              \"val_acc\", \"test_acc\", \"dpo_loss\"])\n",
        "\n",
        "      # get our optimizer\n",
        "      optimizer = AdamW(train_model.parameters(), lr=LR)\n",
        "\n",
        "      #### DATA LOADING\n",
        "\n",
        "      # load in the train/val/test splits for this seed + the pre-computed logits\n",
        "      CLEANED_DIR = f\"/content/drive/MyDrive/cs329h_project/cleaned/{dataset}/seed={seed}\"\n",
        "\n",
        "      # the train/val/test data for this split\n",
        "      train_df = pd.read_csv(f\"{CLEANED_DIR}/data/train.csv\")[\n",
        "          [\"prompt\", \"chosen\", \"rejected\"]]\n",
        "      val_df = pd.read_csv(f\"{CLEANED_DIR}/data/val.csv\")[\n",
        "          [\"prompt\", \"chosen\", \"rejected\"]]\n",
        "      test_df = pd.read_csv(f\"{CLEANED_DIR}/data/test.csv\")[\n",
        "          [\"prompt\", \"chosen\", \"rejected\"]]\n",
        "\n",
        "      # reference train log-probs for this split\n",
        "      ref_train = pd.read_csv(f\"{CLEANED_DIR}/precomputed/{ref_model}_train.csv\")\n",
        "\n",
        "      #### MODEL TRAINING\n",
        "\n",
        "      # training loop over epochs and batches\n",
        "      for epoch in range(N_EPOCHS):\n",
        "        for batch in tqdm(range((NUM_BATCHES))):\n",
        "\n",
        "          # get this mini-batch worth of data\n",
        "          batch_data = train_df.loc[batch * BATCH_SIZE : ((batch+1) * BATCH_SIZE) - 1]\n",
        "          batch_prompt, batch_chosen, batch_rejected = batch_data[\"prompt\"].tolist(), batch_data[\"chosen\"].tolist(), batch_data[\"rejected\"].tolist()\n",
        "\n",
        "          # compute accuracy on this batch before we do the gradient update\n",
        "          with torch.no_grad():\n",
        "            batch_acc_pre = compute_acc(\n",
        "                model=train_model, tok=tok, prompt=batch_prompt, chosen=batch_chosen, rejected=batch_rejected, MAX_BATCH=50).cpu().item()\n",
        "\n",
        "          # compute the current log-probs on the training batch's preferred.\n",
        "          preferred_train_log_probs, L_chosen = CLRL(\n",
        "              model=train_model, tok=tok, prompts=batch_prompt, replies=batch_chosen, device=device)\n",
        "          policy_lpm_chosen = torch.stack(preferred_train_log_probs) / torch.tensor(L_chosen, device=device, dtype=torch.float16)\n",
        "\n",
        "          # compute the current log-probs on the training batch's rejected.\n",
        "          nonpreferred_train_log_probs, L_rejected = CLRL(\n",
        "              model=train_model, tok=tok, prompts=batch_prompt, replies=batch_rejected, device=device)\n",
        "          policy_lpm_rejected = torch.stack(nonpreferred_train_log_probs) / torch.tensor(L_rejected, device=device, dtype=torch.float16)\n",
        "\n",
        "          # get the logprobs and L of the precomputed reference model\n",
        "          ref_lp_chosen = torch.tensor(ref_train.loc[batch_data.index][\"logprob_chosen\"].values, dtype=torch.float16).to(device)\n",
        "          ref_lp_rejected = torch.tensor(ref_train.loc[batch_data.index][\"logprob_rejected\"].values, dtype=torch.float16).to(device)\n",
        "          ref_L_chosen = torch.tensor(ref_train.loc[batch_data.index][\"L_chosen\"].values, dtype=torch.float16).to(device)\n",
        "          ref_L_rejected = torch.tensor(ref_train.loc[batch_data.index][\"L_rejected\"].values, dtype=torch.float16).to(device)\n",
        "          ref_lpm_chosen = ref_lp_chosen / ref_L_chosen\n",
        "          ref_lpm_rejected = ref_lp_rejected / ref_L_rejected\n",
        "\n",
        "          # compute the DPO objective\n",
        "          loss = torch.mean(\n",
        "              -torch.nn.functional.logsigmoid(\n",
        "                  BETA * (\n",
        "                      (policy_lpm_chosen - ref_lpm_chosen) - \\\n",
        "                      (policy_lpm_rejected - ref_lpm_rejected))\n",
        "                  )\n",
        "              )\n",
        "\n",
        "          # backward-pass\n",
        "          loss.backward()\n",
        "\n",
        "          # update our parameters + zero our gradient\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # compute accuracy on this batch AFTER we did the gradient update\n",
        "          with torch.no_grad():\n",
        "            batch_acc_post = compute_acc(\n",
        "                model=train_model, tok=tok, prompt=batch_prompt, chosen=batch_chosen, rejected=batch_rejected, MAX_BATCH=50).cpu().item()\n",
        "\n",
        "          # record 10 batches and guarantee the ending.\n",
        "          if ((batch + 1) % (NUM_BATCHES // 10) == 0) or (batch == (NUM_BATCHES - 1)):\n",
        "\n",
        "            # compute val and test accuracy too.\n",
        "            with torch.no_grad():\n",
        "\n",
        "              # validation set\n",
        "              val_acc = compute_acc(\n",
        "                  train_model, tok, val_df.prompt.values, val_df.chosen.values,\n",
        "                  val_df.rejected.values, MAX_BATCH=50).cpu().item()\n",
        "\n",
        "              # test set acc\n",
        "              test_acc = compute_acc(\n",
        "                  train_model, tok, test_df.prompt.values, test_df.chosen.values,\n",
        "                  test_df.rejected.values, MAX_BATCH=50).cpu().item()\n",
        "              print(test_acc)\n",
        "\n",
        "          else:\n",
        "\n",
        "            # just put nan's.\n",
        "            val_acc, test_acc = np.nan, np.nan\n",
        "\n",
        "          # record our results\n",
        "          logs.loc[len(logs.index)] = [\n",
        "              epoch, batch, batch_acc_pre, batch_acc_post,\n",
        "              val_acc, test_acc, loss.cpu().item()]\n",
        "\n",
        "          # clean house\n",
        "          del batch_data, batch_prompt, batch_chosen, batch_rejected, batch_acc_pre, preferred_train_log_probs, L_chosen, policy_lpm_chosen\n",
        "          del nonpreferred_train_log_probs, L_rejected, policy_lpm_rejected, ref_lp_chosen, ref_lp_rejected, ref_L_chosen, ref_L_rejected, ref_lpm_chosen, ref_lpm_rejected\n",
        "          del loss, batch_acc_post, val_acc, test_acc\n",
        "          gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "      # save our logs at the end\n",
        "      logs.to_csv(\n",
        "          f\"/content/drive/MyDrive/cs329h_project/results/baseline-DPO_dataset={dataset}_rm={ref_model}_seed={seed}.csv\", index=False)\n",
        "\n",
        "      #### CLEAN HOUSE + STATUS UPDATE\n",
        "\n",
        "      # update our counter\n",
        "      counter += 1\n",
        "\n",
        "      # clear memory\n",
        "      del bnb_cfg, target_modules, lora_cfg, train_model, tok, logs, optimizer, train_df, val_df, test_df, ref_train\n",
        "      gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "      # clear output too\n",
        "      clear_output(wait=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
