{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDMs0k7YLInu",
        "outputId": "9ff67095-56c0-4582-fc30-e5401499cfcb"
      },
      "outputs": [],
      "source": [
        "! pip install bitsandbytes==0.46.0 accelerate==1.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uYvE9T6Eall",
        "outputId": "bfcdd3d2-d2fd-4251-a577-5b263f55626f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from transformers import get_scheduler\n",
        "from tqdm.notebook import tqdm\n",
        "import sys, os, gc\n",
        "from IPython.display import clear_output\n",
        "from torch.optim import AdamW\n",
        "from peft import PeftConfig, PeftModel, LoraConfig, get_peft_model\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "# mount google drive + link to the correct folder\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "sys.path.append(\"/content/drive/MyDrive/cs329h_project\")\n",
        "\n",
        "# helper functions\n",
        "from helpers import compute_logprob_and_reply_length_batched as CLRL\n",
        "from helpers import compute_acc\n",
        "from helpers import compute_mrpo_objective\n",
        "from helpers import compute_dpo_objective_many_refs\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "# we will try 2x datasets, 3x seeds, with 7x potential reference models\n",
        "DATASETS = [\n",
        "    \"PKU-SafeRLHF-30K-standard\",\n",
        "    \"ultrafeedback_binarized\"]\n",
        "REF_MODELS = [\n",
        "    \"01-ai_Yi-1.5-9B-Chat\",\n",
        "    \"meta-llama_Meta-Llama-3.1-8B-Instruct\",\n",
        "    \"microsoft_Phi-3-medium-128k-instruct\",\n",
        "    \"mistralai_Mistral-7B-Instruct-v0.3\",\n",
        "    \"Qwen_Qwen2.5-0.5B-Instruct\",\n",
        "    \"Qwen_Qwen2.5-1.5B-Instruct\",\n",
        "    \"Qwen_Qwen3-4B-Instruct-2507\"\n",
        "    ]\n",
        "N_REF_MODELS = len(REF_MODELS)\n",
        "SEEDS = list(range(3))\n",
        "PRIOR_INIT_VALS = [5.0, 10.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bcff89be7a3246a0937e95cf668f6152"
          ]
        },
        "id": "RVbHvGrC4Xl2",
        "outputId": "e282086a-d6ca-4f01-b8d8-507611f9a7af"
      },
      "outputs": [],
      "source": [
        "# keep a counter for checkpointing\n",
        "counter = 0\n",
        "\n",
        "# loop through prior_init_vals, datasets, and seeds\n",
        "for prior_init_val in PRIOR_INIT_VALS:\n",
        "  for dataset in [DATASETS[0]]:\n",
        "    for seed in [3, 4]: # SEEDS:\n",
        "\n",
        "      #### LOADING IN THE TRAINING MODEL\n",
        "\n",
        "      ## configuring BitsandBytes (4bit) + LORA (default) settings.\n",
        "      bnb_cfg = BitsAndBytesConfig(\n",
        "          load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
        "          bnb_4bit_use_double_quant=False, bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "      target_modules = \"q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj\"\n",
        "      lora_cfg = LoraConfig(\n",
        "          r=32, lora_alpha=16, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
        "          init_lora_weights=\"gaussian\", target_modules=target_modules.split(\",\"))\n",
        "\n",
        "      ## getting our base model Qwen2.5-0.5B-Instruct\n",
        "      POLICY_MODEL = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "      train_model = AutoModelForCausalLM.from_pretrained(\n",
        "          POLICY_MODEL, device_map=\"auto\", quantization_config=bnb_cfg, trust_remote_code=True)\n",
        "      train_model.config.use_cache = False\n",
        "      train_model = prepare_model_for_kbit_training(train_model)\n",
        "      train_model = get_peft_model(train_model, lora_cfg)\n",
        "      train_model.train()\n",
        "      tok = AutoTokenizer.from_pretrained(\n",
        "          POLICY_MODEL, trust_remote_code=True, use_fast=True)\n",
        "      torch.cuda.empty_cache()\n",
        "      gc.collect()\n",
        "\n",
        "      # some other settings to reduce memory\n",
        "      train_model.gradient_checkpointing_enable()\n",
        "      train_model.enable_input_require_grads()\n",
        "\n",
        "      #### HYPERPARAMETER SETTINGS\n",
        "\n",
        "      # fixed hyperparameter settings\n",
        "      BETA, N_EPOCHS, LR = 0.1, 1, 1e-4\n",
        "      BATCH_SIZE = 50 if dataset == \"PKU-SafeRLHF-30K-standard\" else 25\n",
        "      STOCH_VAL_SIZE = 250 if dataset == \"PKU-SafeRLHF-30K-standard\" else 125\n",
        "\n",
        "      # compute how many batches we will need\n",
        "      NUM_BATCHES = (5000 // BATCH_SIZE) if (5000 % BATCH_SIZE) == 0 else (5000 // BATCH_SIZE) + 1\n",
        "\n",
        "      # what logs are we trying to record? NO TRAIN_ACC BECAUSE TOO EXPENSIVE\n",
        "      logs = pd.DataFrame(\n",
        "          data=None, columns=[\n",
        "              \"epoch\", \"batch\",\n",
        "              \"batch_acc_pre\", \"batch_acc_post\",\n",
        "              \"stoch_val_acc_pre\", \"stoch_val_acc_post\",\n",
        "              \"chosen_dpo_loss\", \"chosen_reference\",\n",
        "              \"dpo0\", \"dpo1\", \"dpo2\", \"dpo3\", \"dpo4\", \"dpo5\", \"dpo6\",\n",
        "              \"a0\", \"a1\", \"a2\", \"a3\", \"a4\", \"a5\", \"a6\",\n",
        "              \"b0\", \"b1\", \"b2\", \"b3\", \"b4\", \"b5\", \"b6\",\n",
        "              \"val_acc\", \"test_acc\"])\n",
        "\n",
        "      # get our optimizer\n",
        "      optimizer = AdamW(train_model.parameters(), lr=LR)\n",
        "\n",
        "      #### DATA LOADING\n",
        "\n",
        "      # load in the train/val/test splits for this seed + the pre-computed logits\n",
        "      CLEANED_DIR = f\"/content/drive/MyDrive/cs329h_project/cleaned/{dataset}/seed={seed}\"\n",
        "\n",
        "      # the train/val/test data for this split\n",
        "      train_df = pd.read_csv(f\"{CLEANED_DIR}/data/train.csv\")[\n",
        "          [\"prompt\", \"chosen\", \"rejected\"]]\n",
        "      val_df = pd.read_csv(f\"{CLEANED_DIR}/data/val.csv\")[\n",
        "          [\"prompt\", \"chosen\", \"rejected\"]]\n",
        "      test_df = pd.read_csv(f\"{CLEANED_DIR}/data/test.csv\")[\n",
        "          [\"prompt\", \"chosen\", \"rejected\"]]\n",
        "\n",
        "      # load in all seven models' reference log-probs for this train split\n",
        "      all_ref_train = {\n",
        "          i : pd.read_csv(f\"{CLEANED_DIR}/precomputed/{ref_model}_train.csv\")\n",
        "          for (i, ref_model) in enumerate(REF_MODELS)}\n",
        "\n",
        "      #### MODEL TRAINING\n",
        "\n",
        "      # set a seed for reproducibility on the Thompson Sampling\n",
        "      np.random.seed(seed)\n",
        "\n",
        "      # instantiate our a's and b's thompson sampling hyperparameters\n",
        "      a = np.full(shape=(7,), fill_value=prior_init_val)\n",
        "      b = np.full(shape=(7,), fill_value=prior_init_val)\n",
        "\n",
        "      # training loop over epochs and batches\n",
        "      for epoch in range(N_EPOCHS):\n",
        "        for batch in tqdm(range((NUM_BATCHES))):\n",
        "\n",
        "          # get this mini-batch worth of data\n",
        "          batch_data = train_df.loc[batch * BATCH_SIZE : ((batch+1) * BATCH_SIZE) - 1]\n",
        "          batch_prompt, batch_chosen, batch_rejected = batch_data[\"prompt\"].tolist(), batch_data[\"chosen\"].tolist(), batch_data[\"rejected\"].tolist()\n",
        "\n",
        "          # compute accuracy on this batch before we do the gradient update\n",
        "          with torch.no_grad():\n",
        "            batch_acc_pre = compute_acc(\n",
        "                model=train_model, tok=tok, prompt=batch_prompt, chosen=batch_chosen, rejected=batch_rejected, MAX_BATCH=50).cpu().item()\n",
        "\n",
        "          # to facilitate reward computation, we need to generate a random validation set\n",
        "          stoch_val_idxs = np.random.choice(a=1000, size=STOCH_VAL_SIZE, replace=False)\n",
        "          stoch_val_data = val_df.loc[stoch_val_idxs]\n",
        "          stoch_val_prompt, stoch_val_chosen, stoch_val_rejected = stoch_val_data[\"prompt\"].tolist(), stoch_val_data[\"chosen\"].tolist(), stoch_val_data[\"rejected\"].tolist()\n",
        "\n",
        "          # compute our stochastic validation accuracy BEFORE gradient step\n",
        "          with torch.no_grad():\n",
        "            stoch_val_acc_pre = compute_acc(\n",
        "                model=train_model, tok=tok, prompt=stoch_val_prompt, chosen=stoch_val_chosen, rejected=stoch_val_rejected, MAX_BATCH=50).cpu().item()\n",
        "            print(f\"Stoch. Val. Acc. Pre: {stoch_val_acc_pre:.3f}\")\n",
        "\n",
        "          # lists to store our snippets of policy_lpm_chosen and rejected, also our loss\n",
        "          policy_lpm_chosen, policy_lpm_rejected = [], []\n",
        "          loss = 0.0\n",
        "\n",
        "          # which reference model are we going to be aligning to?\n",
        "          chosen_reference = np.argmax(np.random.beta(a, b))\n",
        "\n",
        "          # micro-batch in sizes of 13\n",
        "          MICRO_BATCH_SIZE = 16\n",
        "          N_MICRO_BATCHES = (BATCH_SIZE // MICRO_BATCH_SIZE) if BATCH_SIZE % MICRO_BATCH_SIZE == 0 else (BATCH_SIZE // MICRO_BATCH_SIZE) + 1\n",
        "          for m_batch in range(N_MICRO_BATCHES):\n",
        "\n",
        "            # what are the indices for this mini-batch?\n",
        "            m_batch_start, m_batch_end = m_batch * MICRO_BATCH_SIZE, np.minimum(BATCH_SIZE, (m_batch + 1) * MICRO_BATCH_SIZE)\n",
        "\n",
        "            # compute the current log-probs on the training batch's preferred.\n",
        "            preferred_train_log_probs_m, L_chosen_m = CLRL(\n",
        "                model=train_model, tok=tok,\n",
        "                prompts=batch_prompt[m_batch_start : m_batch_end],\n",
        "                replies=batch_chosen[m_batch_start : m_batch_end], device=device)\n",
        "            policy_lpm_chosen_m = torch.stack(preferred_train_log_probs_m) / torch.tensor(L_chosen_m, device=device, dtype=torch.float16)\n",
        "\n",
        "            # compute the current log-probs on the training batch's rejected.\n",
        "            nonpreferred_train_log_probs_m, L_rejected_m = CLRL(\n",
        "                model=train_model, tok=tok,\n",
        "                prompts=batch_prompt[m_batch_start : m_batch_end],\n",
        "                replies=batch_rejected[m_batch_start : m_batch_end], device=device)\n",
        "            policy_lpm_rejected_m = torch.stack(nonpreferred_train_log_probs_m) / torch.tensor(L_rejected_m, device=device, dtype=torch.float16)\n",
        "\n",
        "            # get the logprobs and L of the CHOSEN precomputed reference model\n",
        "            ref_lp_chosen_m = torch.tensor(\n",
        "                all_ref_train[chosen_reference].loc[batch_data.index][\"logprob_chosen\"].values[m_batch_start : m_batch_end], dtype=torch.float16).to(device)\n",
        "            ref_lp_rejected_m = torch.tensor(\n",
        "                all_ref_train[chosen_reference].loc[batch_data.index][\"logprob_rejected\"].values[m_batch_start : m_batch_end], dtype=torch.float16).to(device)\n",
        "            ref_L_chosen_m = torch.tensor(\n",
        "                all_ref_train[chosen_reference].loc[batch_data.index][\"L_chosen\"].values[m_batch_start : m_batch_end], dtype=torch.float16).to(device)\n",
        "            ref_L_rejected_m = torch.tensor(\n",
        "                all_ref_train[chosen_reference].loc[batch_data.index][\"L_rejected\"].values[m_batch_start : m_batch_end], dtype=torch.float16).to(device)\n",
        "            ref_lpm_chosen_m = ref_lp_chosen_m / ref_L_chosen_m\n",
        "            ref_lpm_rejected_m = ref_lp_rejected_m / ref_L_rejected_m\n",
        "\n",
        "            # compute the DPO objective\n",
        "            loss_m = torch.mean(\n",
        "                -torch.nn.functional.logsigmoid(\n",
        "                    BETA * (\n",
        "                        (policy_lpm_chosen_m - ref_lpm_chosen_m) - \\\n",
        "                        (policy_lpm_rejected_m - ref_lpm_rejected_m))\n",
        "                    )\n",
        "                ) * (m_batch_end - m_batch_start) / BATCH_SIZE\n",
        "\n",
        "            # backward-pass\n",
        "            loss_m.backward()\n",
        "\n",
        "            # add detached versions of policy_lpm_chosen_m, policy_lpm_rejected_m\n",
        "            policy_lpm_chosen.append(policy_lpm_chosen_m.detach())\n",
        "            policy_lpm_rejected.append(policy_lpm_rejected_m.detach())\n",
        "            loss += loss_m.detach()\n",
        "\n",
        "          # update our parameters + zero our gradient for the full batch!\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # get the full tensors\n",
        "          policy_lpm_chosen = torch.cat(policy_lpm_chosen, dim=0)\n",
        "          policy_lpm_rejected = torch.cat(policy_lpm_rejected, dim=0)\n",
        "\n",
        "          # compute accuracy on this batch AFTER we did the gradient update\n",
        "          with torch.no_grad():\n",
        "            batch_acc_post = compute_acc(\n",
        "                model=train_model, tok=tok, prompt=batch_prompt, chosen=batch_chosen, rejected=batch_rejected, MAX_BATCH=50).cpu().item()\n",
        "\n",
        "          # compute our stochastic validation accuracy AFTER the gradient step to determine reward\n",
        "          with torch.no_grad():\n",
        "            stoch_val_acc_post = compute_acc(\n",
        "                model=train_model, tok=tok, prompt=stoch_val_prompt, chosen=stoch_val_chosen, rejected=stoch_val_rejected, MAX_BATCH=50).cpu().item()\n",
        "            print(f\"Stoch. Val. Acc. Post: {stoch_val_acc_post:.3f}\")\n",
        "\n",
        "          # update our Thompson sampling hyperparameters\n",
        "          if stoch_val_acc_post > stoch_val_acc_pre:\n",
        "            a[chosen_reference] += 1\n",
        "          else:\n",
        "            b[chosen_reference] += 1\n",
        "\n",
        "          # record 10 batches and guarantee the ending.\n",
        "          if ((batch + 1) % (NUM_BATCHES // 10) == 0) or (batch == (NUM_BATCHES - 1)):\n",
        "\n",
        "            # compute val and test accuracy too.\n",
        "            with torch.no_grad():\n",
        "\n",
        "              # validation set\n",
        "              val_acc = compute_acc(\n",
        "                  train_model, tok, val_df.prompt.values, val_df.chosen.values,\n",
        "                  val_df.rejected.values, MAX_BATCH=50).cpu().item()\n",
        "\n",
        "              # test set acc\n",
        "              test_acc = compute_acc(\n",
        "                  train_model, tok, test_df.prompt.values, test_df.chosen.values,\n",
        "                  test_df.rejected.values, MAX_BATCH=50).cpu().item()\n",
        "              print(test_acc)\n",
        "\n",
        "          else:\n",
        "\n",
        "            # just put nan's.\n",
        "            val_acc, test_acc = np.nan, np.nan\n",
        "\n",
        "          # also record per-reference model DPO losses\n",
        "          all_ref_lp_chosen = torch.tensor(\n",
        "              np.array([all_ref_train[i].loc[batch_data.index][\"logprob_chosen\"].values\n",
        "                        for i in range(N_REF_MODELS)]).T, dtype=torch.float16).to(device)\n",
        "          all_ref_lp_rejected = torch.tensor(\n",
        "              np.array([all_ref_train[i].loc[batch_data.index][\"logprob_rejected\"].values\n",
        "                        for i in range(N_REF_MODELS)]).T, dtype=torch.float16).to(device)\n",
        "          all_ref_L_chosen = torch.tensor(\n",
        "              np.array([all_ref_train[i].loc[batch_data.index][\"L_chosen\"].values\n",
        "                        for i in range(N_REF_MODELS)]).T, dtype=torch.float16).to(device)\n",
        "          all_ref_L_rejected = torch.tensor(\n",
        "              np.array([all_ref_train[i].loc[batch_data.index][\"L_rejected\"].values\n",
        "                        for i in range(N_REF_MODELS)]).T, dtype=torch.float16).to(device)\n",
        "          all_ref_lpm_chosen = all_ref_lp_chosen / all_ref_L_chosen\n",
        "          all_ref_lpm_rejected = all_ref_lp_rejected / all_ref_L_rejected\n",
        "          with torch.no_grad():\n",
        "            per_reference_dpo_loss = compute_dpo_objective_many_refs(\n",
        "                policy_lpm_chosen, policy_lpm_rejected,\n",
        "                all_ref_lpm_chosen, all_ref_lpm_rejected, BETA)\n",
        "\n",
        "          # record our results\n",
        "          logs.loc[len(logs.index)] = [\n",
        "              epoch, batch,\n",
        "              batch_acc_pre, batch_acc_post,\n",
        "              stoch_val_acc_pre, stoch_val_acc_post,\n",
        "              loss.cpu().item(), chosen_reference]\\\n",
        "               + list(per_reference_dpo_loss.cpu().numpy())\\\n",
        "               + list(a) + list(b) + [val_acc, test_acc]\n",
        "\n",
        "          # clean house\n",
        "          del batch_data, batch_prompt, batch_chosen, batch_rejected, batch_acc_pre\n",
        "          del stoch_val_idxs, stoch_val_data, stoch_val_prompt, stoch_val_chosen, stoch_val_rejected, stoch_val_acc_pre\n",
        "          del preferred_train_log_probs_m, L_chosen_m, policy_lpm_chosen_m, policy_lpm_chosen\n",
        "          del nonpreferred_train_log_probs_m, L_rejected_m, policy_lpm_rejected_m, policy_lpm_rejected\n",
        "          del chosen_reference, ref_lp_chosen_m, ref_lp_rejected_m, ref_L_chosen_m, ref_L_rejected_m, ref_lpm_chosen_m, ref_lpm_rejected_m\n",
        "          del loss_m, loss, batch_acc_post, stoch_val_acc_post, val_acc, test_acc\n",
        "          del all_ref_lp_chosen, all_ref_lp_rejected, all_ref_L_chosen, all_ref_L_rejected, all_ref_lpm_chosen, all_ref_lpm_rejected\n",
        "          del per_reference_dpo_loss\n",
        "          gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "      # save our logs at the end\n",
        "      logs.to_csv(\n",
        "        f\"/content/drive/MyDrive/cs329h_project/results/Thompson-DPO_dataset={dataset}_PIV={prior_init_val}_seed={seed}.csv\", index=False)\n",
        "\n",
        "      #### CLEAN HOUSE + STATUS UPDATE\n",
        "\n",
        "      # update our counter\n",
        "      counter += 1\n",
        "\n",
        "      # clear memory\n",
        "      del bnb_cfg, target_modules, lora_cfg, train_model, tok, logs, optimizer\n",
        "      del train_df, val_df, test_df, all_ref_train, a, b\n",
        "      gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "      # clear output too\n",
        "      clear_output(wait=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
